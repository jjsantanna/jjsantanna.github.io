---
layout: post
title:  "Attacks on AI LLM"
date:   2024-09-05
image:  20240905_ai_scared.jpeg
tags:   [Jekyll]
---

The goal of this post is to list adversarial attacks against AI LLM (Chatbots and APIs). Let me know if you know more in the comments bellow.

## Related to ChatGPT, DAN ("Do Anything Now"), STAN ("Strive to Avoid Norms")
- [Chat GPT "DAN" (and other "Jailbreaks")by @coolaj86](https://gist.github.com/coolaj86/6f4f7b30129b0251f61fa7baaa881516)
- https://hix.ai/hub/chatgpt/how-to-jailbreak-chatgpt
- https://github.com/alexisvalentino/Chatgpt-DAN
- https://github.com/0xk1h0/ChatGPT_DAN
- https://www.reddit.com/r/ChatGPT/comments/11dvjzh/dan_90_the_newest_jailbreak/

## Related to Microsoft Copilot:
- [Conditional Prompt Injection Attacks with Microsoft Copilot by @Johann Rehberger (@wunderwuzzi)] (https://embracethered.com/blog/posts/2024/whoami-conditional-prompt-injection-instructions/)

## Game and Tests
- [GPT Prompt Attack](https://gpa.43z.one/)
